## Logistic 回归

- Sigmoid函数
- 梯度上升（或者下降）

在两个类别的情况下，我们想要函数能够输出0，1。常见的就是分段函数。但是他有跳跃，在跳跃点不可导。

sigmoid函数。
$$
sigmoid(x) = \frac{1}{1+e^{-x}}
$$


梯度上升：

我们最常见的应该是梯度下降，那是我们在设计损失函数时使用，让损失函数最小化。梯度上升是让我们的目标函数最大化。其优化方式还有很多。

文章中第一个例子，求导时，采用计算全部样本的误差，更新权重。

以此，我们有了sgd，等等优化方案。

```python
# z = w0*x0 + w1*x1 + b
def grad(data, label):
    # np.mat() 将输入转成矩阵，如果输入是array，则不会复制，会共享内存
    # np.mat().transpose()矩阵的转置
    dataMatrix = np.mat(data)         # 转成矩阵 n*3
    labelMatrix = np.mat(label).transpose() # n*1
    m, n = dataMatrix.shape
    alpha = 0.001
    epoch = 500
    weight = np.ones((n,1))     # 3*1

    # 此处weight的更新
    for k in range(epoch):
        h = sigmoid(dataMatrix*weight)
        error = labelMatrix - h
        weight = weight + alpha * dataMatrix.transpose()*error
    return weight
```





