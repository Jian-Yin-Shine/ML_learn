### CART 

classification and reression tree(分类回归树)，简称CART。

《机器学习实战》

相比于前面的决策树来说，利用信息增益，找到使信息增益最大的特征，进行切分，此特征就没有用了。

- 有观点认为这样的切分方式太过于迅速。

- ID3不能直接处理连续型特征。只有事先将连续型特征转换为离散型，这样会破坏连续型变量的内在性质。



| 不浮出水面是否可以生存 | 是否有脚蹼 | 属于鱼类 |
| ---------------------- | ---------- | -------- |
| 是                     | 是         | 是       |
| 是                     | 是         | 是       |
| 是                     | 否         | 否       |
| 否                     | 是         | 否       |
| 否                     | 是         | 否       |

信息熵：$H=-\sum_{i=1}^{n}p(x_i)log_2p(x_i)$  ，其中数据集中某个类别的概率。



- CART回归树，解决了上述两个问题。

选择最优切分变量（特征）以及切分点（某个特征的值），使得：


$$
min_{j,s}[min_{c_1}\sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
$$
其中：
$$
R_1(j,s) = \{x | x^{(j)} \le s \}, R_2(j,s) = \{x | x^{(j)} > s\}
$$

$$
c'_m = \frac{1}{N_m} \sum_{x_i \in R_m(j,s)} y_i      , m=1,2
$$

看上去很复杂，其实用语言描述其流程就是：

1） 循环遍历每一个特征的特征值，按值切分为两个子集，根据Y，找到最小的$\sum (y-mean(y))^2$ 

2）标记出特征，和特征值，然后切分（注意，切分特征保留）

3）重复此过程，直到设定的条件（子集Y全部一样，子集样本太少，划分没有意义即误差减少太少等）